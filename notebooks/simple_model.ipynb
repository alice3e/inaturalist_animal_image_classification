{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dvclive import Live\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image, ImageFile\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f3b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../params.yaml', 'r') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "print(\"Parameters:\")\n",
    "print(yaml.dump(params, default_flow_style=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6974ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/balanced_animals_dataset.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_idx = {label: idx for idx, label in enumerate(df['scientific_name'].unique())}\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "df['label'] = df['scientific_name'].map(label_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a84d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df, \n",
    "    test_size=params['data']['train_test_split'], \n",
    "    stratify=df['label'], \n",
    "    random_state=params['data']['random_state']\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e973b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.base_path = Path('../animal_images')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        species = row['scientific_name'].replace(' ', '_')\n",
    "        img_path = self.base_path / species / f\"{row['uuid']}.jpg\"\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception:\n",
    "            image = Image.new('RGB', (224, 224), color=(0, 0, 0))\n",
    "            \n",
    "        label = row['label']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b59aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(params['augmentation']['resize']),\n",
    "    transforms.RandomCrop(params['augmentation']['crop_size']),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=params['augmentation']['brightness'],\n",
    "        contrast=params['augmentation']['contrast'],\n",
    "        saturation=params['augmentation']['contrast']\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(params['augmentation']['resize']),\n",
    "    transforms.CenterCrop(params['augmentation']['crop_size']),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f23d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AnimalDataset(train_df, transform=train_transform)\n",
    "val_dataset = AnimalDataset(val_df, transform=val_transform)\n",
    "\n",
    "batch_size = params['training']['batch_size']\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d20f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['model']['name'] == 'resnet50':\n",
    "    weights = getattr(models.ResNet50_Weights, params['model']['pretrained'])\n",
    "    model = models.resnet50(weights=weights)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, params['model']['num_classes'])\n",
    "    \n",
    "elif params['model']['name'] == 'resnet101':\n",
    "    weights = getattr(models.ResNet101_Weights, params['model']['pretrained'])\n",
    "    model = models.resnet101(weights=weights)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, params['model']['num_classes'])\n",
    "    \n",
    "elif params['model']['name'] == 'efficientnet_v2_m':\n",
    "    weights = getattr(models.EfficientNet_V2_M_Weights, params['model']['pretrained'])\n",
    "    model = models.efficientnet_v2_m(weights=weights)\n",
    "    num_features = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(num_features, params['model']['num_classes'])\n",
    "    \n",
    "elif params['model']['name'] == 'convnext_base':\n",
    "    weights = getattr(models.ConvNeXt_Base_Weights, params['model']['pretrained'])\n",
    "    model = models.convnext_base(weights=weights)\n",
    "    num_features = model.classifier[2].in_features\n",
    "    model.classifier[2] = nn.Linear(num_features, params['model']['num_classes'])\n",
    "    \n",
    "elif params['model']['name'] == 'vit_b_16':\n",
    "    weights = getattr(models.ViT_B_16_Weights, params['model']['pretrained'])\n",
    "    model = models.vit_b_16(weights=weights)\n",
    "    num_features = model.heads.head.in_features\n",
    "    model.heads.head = nn.Linear(num_features, params['model']['num_classes'])\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Разморозить последний слой\n",
    "if 'efficientnet' in params['model']['name']:\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "elif 'convnext' in params['model']['name']:\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "elif 'vit' in params['model']['name']:\n",
    "    for param in model.heads.parameters():\n",
    "        param.requires_grad = True\n",
    "else:\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model: {params['model']['name']}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63049f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Получить параметры для обучения в зависимости от модели\n",
    "if 'efficientnet' in params['model']['name']:\n",
    "    trainable_params = model.classifier.parameters()\n",
    "elif 'convnext' in params['model']['name']:\n",
    "    trainable_params = model.classifier.parameters()\n",
    "elif 'vit' in params['model']['name']:\n",
    "    trainable_params = model.heads.parameters()\n",
    "else:\n",
    "    trainable_params = model.fc.parameters()\n",
    "\n",
    "optimizer = optim.Adam(trainable_params, lr=params['training']['learning_rate'])\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer, \n",
    "    step_size=params['scheduler']['step_size'], \n",
    "    gamma=params['scheduler']['gamma']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61083b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd78141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total, np.array(all_preds), np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2804810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = params['training']['num_epochs']\n",
    "best_val_acc = 0.0\n",
    "Path('../models').mkdir(exist_ok=True)\n",
    "\n",
    "with Live(dir='../dvclive', save_dvc_exp=True) as live:\n",
    "    \n",
    "    live.log_params(params)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        val_loss, val_acc, val_preds, val_labels = validate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        live.log_metric('train/loss', train_loss)\n",
    "        live.log_metric('train/accuracy', train_acc)\n",
    "        live.log_metric('val/loss', val_loss)\n",
    "        live.log_metric('val/accuracy', val_acc)\n",
    "        live.next_step()\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'label_to_idx': label_to_idx,\n",
    "                'idx_to_label': idx_to_label,\n",
    "                'params': params\n",
    "            }, '../models/best_model.pth')\n",
    "            print(f\"Best model saved: {val_acc:.2f}%\")\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(val_labels, val_preds)\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=[idx_to_label[i] for i in range(len(idx_to_label))],\n",
    "        yticklabels=[idx_to_label[i] for i in range(len(idx_to_label))],\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    confusion_matrix_path = '../dvclive/plots/confusion_matrix.png'\n",
    "    Path(confusion_matrix_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(confusion_matrix_path, dpi=150, bbox_inches='tight')\n",
    "    live.log_image('confusion_matrix.png', confusion_matrix_path)\n",
    "    plt.close()\n",
    "    \n",
    "    live.log_metric('best_val_accuracy', best_val_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
